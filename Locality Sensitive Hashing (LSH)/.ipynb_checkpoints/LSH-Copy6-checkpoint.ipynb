{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk import ngrams\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import random\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data file couldn't be checked into github because it is too large instead it must be downloaded here #https://www.dropbox.com/s/ir6he8jxxagugnw/assignment3_aricles.json?dl=0datafile =  'data/assignment3_aricles.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_json('assignment3_aricles.json', orient='records', encoding=\"utf-8\")\n",
    "list_of_primes = np.array(pd.read_csv('P-1000000.txt', sep=\",\", header=None))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can use n-gram at word level for this task\n",
    "#try with different n-gram values \n",
    "# You can use ngrams from nltk for this\n",
    "\n",
    "def getNgrams(articles,K):\n",
    "    # Param type_split : the type of splitting is it by word or number of character\n",
    "    print('Getting Ngrams for all articles')\n",
    "    Ngrams_lists = []\n",
    "    n_articles=len(articles)\n",
    "\n",
    "    for article in articles:\n",
    "        # convert to lower case\n",
    "        article = article.lower()\n",
    "        # Replace all none alphanumeric characters with spaces\n",
    "        article = re.sub(r'[^a-zA-Z0-9\\s]', ' ', article)\n",
    "        #article = re.sub(r\"[^\\w\\s]\", ' ', article)\n",
    "        article = article.replace('\\n',' ')\n",
    "        words = [word for word in article.split(' ') if word !='']\n",
    "        length_words = len(words)\n",
    "        Ngrams_lists.append([' '.join(words[i:i+K]) for i in range(0,length_words) if i+K<=length_words])\n",
    "        \n",
    "    #return list of ngrams for each article\n",
    "    return Ngrams_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert n-grams into binary vector representation for each document. You can do some optimzations if the matrix is too big.\n",
    "For example, \n",
    "* Select top 10000 most frequent n-grams.\n",
    "* You may also try smaller values of n (like 2 or 3) which result in fewer n-grams.\n",
    "* Finally, you can also try sparse matrix representation. Like csr_matrix from  scipy.sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinaryMatrix(articles_ngram,partial= True):\n",
    "    #parameter \n",
    "    #partial = True -> select top 10000 shingles, False -> all shingles\n",
    "    \n",
    "    shingles_id = {}\n",
    "    shingles_count = {}\n",
    "    binary_matrix2 = {}\n",
    "    \n",
    "    print('Creating shingles_id and binary matrix')\n",
    "    for i in range(len(articles_ngram)):\n",
    "        for j in range(len(articles_ngram[i])):\n",
    "            shingle = articles_ngram[i][j]\n",
    "            if len(shingles_id)==0:\n",
    "                shingles_id[shingle] = 0\n",
    "                binary_matrix2[0] = [i]\n",
    "                shingles_count[0] = 1\n",
    "            elif shingle in shingles_id:\n",
    "                shing_id = shingles_id[shingle]\n",
    "                shingles_count[shing_id] = shingles_count[shing_id]+1\n",
    "                if i not in binary_matrix2[shing_id]:\n",
    "                    binary_matrix2[shing_id].append(i)\n",
    "            else:\n",
    "                new_id = len(shingles_id)\n",
    "                shingles_id[shingle] = new_id\n",
    "                binary_matrix2[new_id] = [i]\n",
    "                shingles_count[new_id] = 1\n",
    "    \n",
    "    #Run if partial == true\n",
    "    if partial==True:\n",
    "        n_top = 10000\n",
    "        print('sorting shingles based on frequency and get the top 10000')\n",
    "        sorted_shingles = sorted(shingles_count.items(),key=operator.itemgetter(1),reverse=True)[:n_top]\n",
    "        print('Creating subset binary matrix')\n",
    "        bin_matrix_subset = {}\n",
    "        shingles_id_subset = {}\n",
    "        \n",
    "        shingles_list = list(shingles_id.keys())\n",
    "        for i in range(n_top):\n",
    "            s_id = sorted_shingles[i][0]\n",
    "            bin_matrix_subset[i] = binary_matrix2[s_id]\n",
    "            \n",
    "            shingles_id_subset[shingles_list[s_id]] = i\n",
    "        \n",
    "        #bin_matrix_subset = {key:binary_matrix2[key] for key in top_n_shingles}\n",
    "        return bin_matrix_subset,shingles_id_subset\n",
    "    \n",
    "    #Run if partial == False    \n",
    "    else:\n",
    "        return binary_matrix2,shingles_id\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1. It might be impossible to avoid collisions but as long as the collions are too many it won't matter much.\n",
    "\n",
    "* The simplest would be using the builtin hash() function, it can be for example, hash(rownumber) % Numberofbuckets\n",
    "* You can generate several of these hash functions by xoring a random integer (hash(rownumber)^randint) % Numberofbuckets\n",
    "* It can also be a as simple as (rownumber * randint) % Numberofbuckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Hash Eq\n",
    "The coefficients a and b are randomly chosen integers less than the maximum value of x. c is a prime number slightly bigger than the maximum value of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_prime(number):\n",
    "    return list_of_primes[list_of_primes>number][0]\n",
    "\n",
    "def getHashFunctionValues(numrows, numhashfunctions=200):\n",
    "    #return a matrix with hash values\n",
    "    print('getting hash values')\n",
    "    hashvalues = []\n",
    "    \n",
    "    #using universal hash function ax+b mod c\n",
    "    #a is any odd number between 1 to next_prime-1(inclusive)\n",
    "    #b is any number between 0 to next_prime -1 (inclusive)\n",
    "    #c is maximum possible value for the hash code + 1\n",
    "    #next_prime is prime number that is greater than max possible value of x \n",
    "    \n",
    "    next_prime = get_next_prime(numrows-1)\n",
    "    print('prime number is=',next_prime)\n",
    "    a = np.array(random.choices(range(1,next_prime,2),k=numhashfunctions))\n",
    "    b = np.array(random.sample(list(range(0,next_prime)),numhashfunctions))\n",
    "    c = np.array(np.ones((numhashfunctions,))*numrows)\n",
    "    \n",
    "    print('constructing hash value matrix')\n",
    "    \n",
    "    for i in range(numrows):\n",
    "        hashvalues.append(((a*i + b)% c).astype(int))\n",
    "        \n",
    "    return np.array(hashvalues).reshape(numrows,numhashfunctions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute minhash following the faster algorithm from the lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMinHashSignatureMatrix(binary_matrix, hash_val_matrix,n_doc):\n",
    "    #return minhash signature matrix\n",
    "    print('Creating Signature Matrix...')\n",
    "    minhash_signature = np.array(np.ones((hash_val_matrix.shape[1],n_doc)) * np.inf)\n",
    "\n",
    "    for key in binary_matrix:\n",
    "        for index in binary_matrix[key]:\n",
    "            minhash_signature[:,index] = np.minimum(minhash_signature[:,index],hash_val_matrix[key])\n",
    "    \n",
    "    #replace the infinity to max values in the matrix +1 just to avoid problem in the mathematic steps further\n",
    "    #minhash_signature[minhash_signature == np.inf] = minhash_signature[minhash_signature!=np.inf].max()+1\n",
    "    \n",
    "    return minhash_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash signature bands into buckets. Find a way to combine all the signature values in a band and hash them into a number of buckets ususally very high.\n",
    "* Easiest way is to add all the signature values in the bucket and use a similar hash function like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLSH(signature_matrix, num_bands, num_buckets): #removing hashfunctions from the parameter \n",
    "    #return lsh buckets or hash table\n",
    "    print('Hashing into buckets...')\n",
    "    buckets = {}\n",
    "    \n",
    "    r = int(signature_matrix.shape[0]/num_bands)\n",
    "    \n",
    "    #using same universal hashing\n",
    "    next_prime = get_next_prime(r-1)\n",
    "    a = random.choices(range(1,next_prime,2),k=r)\n",
    "    b = random.randint(0,next_prime-1)\n",
    "    c = num_buckets\n",
    "    \n",
    "    for i in range(num_bands):\n",
    "        each_band = signature_matrix[i*r:(i*r)+r]\n",
    "        bucket_nos = ((np.dot(a,each_band)+b)%c).astype(int)\n",
    "        for j in range(len(bucket_nos)):\n",
    "            if bucket_nos[j] in buckets:\n",
    "                buckets[bucket_nos[j]].append(j)\n",
    "            else:\n",
    "                buckets[bucket_nos[j]]= [j]\n",
    "    print('Done Hashing')\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune parameters to make sure the threshold is appropriate.\n",
    "## plot the probability of two similar items falling in same bucket for different threshold values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(s,b,r):\n",
    "    prob = 1-(1-s**r)**b\n",
    "    return prob\n",
    "\n",
    "#pairs of number of bands and rows\n",
    "parameters = [[100,2],[50,4],[40,5],[20,10],[10,20],[5,40],[4,50],[2,100]]\n",
    "n_hash = 200\n",
    "prob = []\n",
    "\n",
    "s = np.linspace(0,1,200)\n",
    "for parameter in parameters:\n",
    "    prob.append(get_prob(s,parameter[0],parameter[1]))\n",
    "\n",
    "#Plotting each stock dataset in the list\n",
    "plot_color=[\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\"]\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for j in range(len(prob)):\n",
    "    ax.plot(s, prob[j], label=str(n_hash)+','+str(parameters[j][0])+','+str(parameters[j][1]))\n",
    "ax.legend(loc='best',title=\"N_hash, bands, rows\")\n",
    "    \n",
    "print ('Best parameters will be : n_hash=',n_hash,',n_bands = 20, n_rows = 10')\n",
    "print('With threshold = ', (1/20)**(1/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters to tune\n",
    "n_bands =20\n",
    "n_rows =10\n",
    "n_hash = n_bands*n_rows\n",
    "num_buckets = 2000000\n",
    "K = 3 #for the ngrams\n",
    "\n",
    "# Get all the n_grams for each articles\n",
    "articles_ngram = getNgrams(articles.iloc[:,0].to_list(),K)\n",
    "\n",
    "#construct binary_matrix (using new function),\n",
    "#if partial is True, then it will take 10000 top occuring shingles, if partial is False, then all shingles will be selected\n",
    "binary_matrix,shingles_id = getBinaryMatrix(articles_ngram,partial=True)\n",
    "nrow = len(binary_matrix)\n",
    "\n",
    "#construct binary_matrix\n",
    "#binary_matrix,nrow = getBinaryMatrix(articles_ngram)\n",
    "hash_values = getHashFunctionValues(nrow,n_hash)\n",
    "\n",
    "signature_matrix = getMinHashSignatureMatrix(binary_matrix,hash_values,articles.shape[0])\n",
    "buckets = getLSH(signature_matrix,n_bands,num_buckets)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best parameters and get nearest neighbors of each articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from operator import itemgetter\n",
    "\n",
    "def get_nearest_neighbour(num_doc,buckets,binary_matrix,threshold):\n",
    "    print('permutation')\n",
    "    pairs_permutations = []\n",
    "    for key in buckets.keys():\n",
    "        unique = set(buckets[key])\n",
    "        if len(unique)>1:\n",
    "            pairs= list(permutations(unique,2))\n",
    "            pairs_permutations.append(pairs)\n",
    "    \n",
    "    print('get all possible pairs')\n",
    "    all_possible_pairs = [x for pairs in pairs_permutations for x in pairs]\n",
    "    \n",
    "    print('Getting sorted unique pairs')\n",
    "    unique_possible_pairs = set(all_possible_pairs)\n",
    "    sorted_unique_pairs = sorted(unique_possible_pairs,key=itemgetter(0))\n",
    "    \n",
    "    print('Create Sparse matrix')\n",
    "    binary_sparse= np.zeros((len(binary_matrix),num_doc)).astype(int)\n",
    "    \n",
    "    for key in binary_matrix.keys():\n",
    "        for item in binary_matrix[key]:\n",
    "            binary_sparse[key][item] = 1\n",
    "\n",
    "    \n",
    "    print('counting jaccard similarity')\n",
    "    nearest_neighbour = []\n",
    "    for pair in sorted_unique_pairs:\n",
    "        c1 = binary_sparse[:,pair[0]]\n",
    "        c2 = binary_sparse[:,pair[1]]\n",
    "        similarity = sum(c1&c2)/sum(c1|c2)\n",
    "        print('similarity = ', similarity)\n",
    "        if similarity>= threshold:\n",
    "            nearest_neighbour.append(pair)\n",
    "    \n",
    "    return nearest_neighbour\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbour2(num_doc,buckets,binary_matrix,threshold):\n",
    "    print('Create Sparse matrix..')\n",
    "    binary_sparse= np.zeros((len(binary_matrix),num_doc)).astype(int)\n",
    "    print('Shape of binary_sparse matrix =',binary_sparse.shape)\n",
    "    \n",
    "    for key in binary_matrix.keys():\n",
    "        for item in binary_matrix[key]:\n",
    "            binary_sparse[key][item] = 1\n",
    "    \n",
    "    print('Finding Nearest Neighbour..')\n",
    "    nearest_neighbour = []\n",
    "    for key in buckets.keys():\n",
    "        if key >= 0:\n",
    "            unique_list = list(set(buckets[key]))\n",
    "            n = len(unique_list)\n",
    "            if n>1:\n",
    "                for i in range (n-1):\n",
    "                    for j in range(i+1,n):\n",
    "                        c1 = binary_sparse[:,unique_list[i]]\n",
    "                        c2 = binary_sparse[:,unique_list[j]]\n",
    "                        similarity = sum(c1&c2)/sum(c1|c2)\n",
    "                        if similarity>=threshold:\n",
    "                            nearest_neighbour.append([unique_list[i],unique_list[j]])\n",
    "    return nearest_neighbour\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour = get_nearest_neighbour2(articles.shape[0],buckets,binary_matrix,threshold=0.74)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_dict = collections.defaultdict(list)\n",
    "for k, v in sorted_unique_pairs:\n",
    "    pairs_dict[k].append(v)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pairs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs_neighbours = []\n",
    "\n",
    "for key in pairs_dict:\n",
    "    document_neighbors = [key]\n",
    "    similarities = []\n",
    "    \n",
    "    for item in pairs_dict[key]:\n",
    "        n_all_sets = len(set(np.concatenate([articles_ngram[key],articles_ngram[item]])))\n",
    "        similarity = len(set(articles_ngram[key]).intersection(set(articles_ngram[item])))/n_all_sets\n",
    "        similarities.append((item,similarity))\n",
    "    \n",
    "    similarities.sort(key=operator.itemgetter(1),reverse=True)\n",
    "    for i in range(len(similarities)):\n",
    "        document_neighbors.append(similarities[i][0])\n",
    "\n",
    "    all_docs_neighbours.append(document_neighbors)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,0,0,0,0])\n",
    "b =np.array([0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(a&b)/sum(a|b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the nearest neibhors of each document to submissions.csv (comma separated, first column is the current document followed by a list of nearest neighbors) file and get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "1 5\n",
      "1 6\n",
      "1 7\n",
      "1 8\n",
      "1 9\n",
      "1 10\n",
      "1 11\n",
      "1 12\n",
      "1 13\n",
      "1 14\n",
      "1 15\n",
      "1 16\n",
      "1 17\n",
      "1 18\n",
      "1 19\n",
      "1 20\n",
      "1 21\n",
      "1 22\n",
      "1 23\n",
      "1 24\n",
      "2 3\n",
      "2 4\n",
      "2 5\n",
      "2 6\n",
      "2 7\n",
      "2 8\n",
      "2 9\n",
      "2 10\n",
      "2 11\n",
      "2 12\n",
      "2 13\n",
      "2 14\n",
      "2 15\n",
      "2 16\n",
      "2 17\n",
      "2 18\n",
      "2 19\n",
      "2 20\n",
      "2 21\n",
      "2 22\n",
      "2 23\n",
      "2 24\n",
      "3 4\n",
      "3 5\n",
      "3 6\n",
      "3 7\n",
      "3 8\n",
      "3 9\n",
      "3 10\n",
      "3 11\n",
      "3 12\n",
      "3 13\n",
      "3 14\n",
      "3 15\n",
      "3 16\n",
      "3 17\n",
      "3 18\n",
      "3 19\n",
      "3 20\n",
      "3 21\n",
      "3 22\n",
      "3 23\n",
      "3 24\n",
      "4 5\n",
      "4 6\n",
      "4 7\n",
      "4 8\n",
      "4 9\n",
      "4 10\n",
      "4 11\n",
      "4 12\n",
      "4 13\n",
      "4 14\n",
      "4 15\n",
      "4 16\n",
      "4 17\n",
      "4 18\n",
      "4 19\n",
      "4 20\n",
      "4 21\n",
      "4 22\n",
      "4 23\n",
      "4 24\n",
      "5 6\n",
      "5 7\n",
      "5 8\n",
      "5 9\n",
      "5 10\n",
      "5 11\n",
      "5 12\n",
      "5 13\n",
      "5 14\n",
      "5 15\n",
      "5 16\n",
      "5 17\n",
      "5 18\n",
      "5 19\n",
      "5 20\n",
      "5 21\n",
      "5 22\n",
      "5 23\n",
      "5 24\n",
      "6 7\n",
      "6 8\n",
      "6 9\n",
      "6 10\n",
      "6 11\n",
      "6 12\n",
      "6 13\n",
      "6 14\n",
      "6 15\n",
      "6 16\n",
      "6 17\n",
      "6 18\n",
      "6 19\n",
      "6 20\n",
      "6 21\n",
      "6 22\n",
      "6 23\n",
      "6 24\n",
      "7 8\n",
      "7 9\n",
      "7 10\n",
      "7 11\n",
      "7 12\n",
      "7 13\n",
      "7 14\n",
      "7 15\n",
      "7 16\n",
      "7 17\n",
      "7 18\n",
      "7 19\n",
      "7 20\n",
      "7 21\n",
      "7 22\n",
      "7 23\n",
      "7 24\n",
      "8 9\n",
      "8 10\n",
      "8 11\n",
      "8 12\n",
      "8 13\n",
      "8 14\n",
      "8 15\n",
      "8 16\n",
      "8 17\n",
      "8 18\n",
      "8 19\n",
      "8 20\n",
      "8 21\n",
      "8 22\n",
      "8 23\n",
      "8 24\n",
      "9 10\n",
      "9 11\n",
      "9 12\n",
      "9 13\n",
      "9 14\n",
      "9 15\n",
      "9 16\n",
      "9 17\n",
      "9 18\n",
      "9 19\n",
      "9 20\n",
      "9 21\n",
      "9 22\n",
      "9 23\n",
      "9 24\n",
      "10 11\n",
      "10 12\n",
      "10 13\n",
      "10 14\n",
      "10 15\n",
      "10 16\n",
      "10 17\n",
      "10 18\n",
      "10 19\n",
      "10 20\n",
      "10 21\n",
      "10 22\n",
      "10 23\n",
      "10 24\n",
      "11 12\n",
      "11 13\n",
      "11 14\n",
      "11 15\n",
      "11 16\n",
      "11 17\n",
      "11 18\n",
      "11 19\n",
      "11 20\n",
      "11 21\n",
      "11 22\n",
      "11 23\n",
      "11 24\n",
      "12 13\n",
      "12 14\n",
      "12 15\n",
      "12 16\n",
      "12 17\n",
      "12 18\n",
      "12 19\n",
      "12 20\n",
      "12 21\n",
      "12 22\n",
      "12 23\n",
      "12 24\n",
      "13 14\n",
      "13 15\n",
      "13 16\n",
      "13 17\n",
      "13 18\n",
      "13 19\n",
      "13 20\n",
      "13 21\n",
      "13 22\n",
      "13 23\n",
      "13 24\n",
      "14 15\n",
      "14 16\n",
      "14 17\n",
      "14 18\n",
      "14 19\n",
      "14 20\n",
      "14 21\n",
      "14 22\n",
      "14 23\n",
      "14 24\n",
      "15 16\n",
      "15 17\n",
      "15 18\n",
      "15 19\n",
      "15 20\n",
      "15 21\n",
      "15 22\n",
      "15 23\n",
      "15 24\n",
      "16 17\n",
      "16 18\n",
      "16 19\n",
      "16 20\n",
      "16 21\n",
      "16 22\n",
      "16 23\n",
      "16 24\n",
      "17 18\n",
      "17 19\n",
      "17 20\n",
      "17 21\n",
      "17 22\n",
      "17 23\n",
      "17 24\n",
      "18 19\n",
      "18 20\n",
      "18 21\n",
      "18 22\n",
      "18 23\n",
      "18 24\n",
      "19 20\n",
      "19 21\n",
      "19 22\n",
      "19 23\n",
      "19 24\n",
      "20 21\n",
      "20 22\n",
      "20 23\n",
      "20 24\n",
      "21 22\n",
      "21 23\n",
      "21 24\n",
      "22 23\n",
      "22 24\n",
      "23 24\n"
     ]
    }
   ],
   "source": [
    "n = 25\n",
    "for i in range (n-1): \n",
    "    for j in range ( i+1,n): \n",
    "        print(i,j) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(buckets.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[np.inf,2,3],[np.inf,5,6],[np.inf,8,9],[np.inf,11,12]]).reshape(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[inf,  2.,  3.],\n",
       "       [inf,  5.,  6.],\n",
       "       [inf,  8.,  9.],\n",
       "       [inf, 11., 12.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7481456661528606911"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(tuple((x[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
